{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall\n",
    "py\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def write_(text):\n",
    "    with open(\"debug.txt\", \"a\") as file:\n",
    "        file.write(f\"python - {text}\\n\")\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, params, number_of_actions):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.f1 = nn.Linear(params['state_size'], params['first_layer_size']) # theta\n",
    "        self.f2 = nn.Linear(params['first_layer_size'], params['second_layer_size'])\n",
    "        self.f3 = nn.Linear(params['second_layer_size'], params['third_layer_size'])\n",
    "        self.f4 = nn.Linear(params['third_layer_size'], number_of_actions)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.f1(x))\n",
    "        x = F.relu(self.f2(x))\n",
    "        x = F.relu(self.f3(x))\n",
    "        x = self.f4(x)\n",
    "        return x\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Kaiming (He) Uniform initialization\n",
    "                nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, params, actions):\n",
    "        super().__init__()\n",
    "        self.number_of_actions = len(actions)\n",
    "        self.actions = actions\n",
    "        self.gamma = params['gamma']\n",
    "        self.short_memory = np.array([])\n",
    "        self.memory = collections.deque(maxlen=params['memory_size'])\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.model = QNetwork(params, self.number_of_actions)\n",
    "        self.model.to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=params['learning_rate'])\n",
    "        self.target_model = QNetwork(params, self.number_of_actions)\n",
    "        self.target_model.to(DEVICE)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model_update_iterations = params['target_model_update_iterations']\n",
    "        self.current_iteration = 0\n",
    "\n",
    "    def on_new_sample(self, state, action, reward, next_state, is_done):\n",
    "        for i, value in enumerate(self.actions):\n",
    "            if value == action:\n",
    "                action_index = i\n",
    "                break\n",
    "        self.memory.append((state, action_index, reward, next_state, is_done))\n",
    "\n",
    "    def replay_mem(self, batch_size):\n",
    "        if len(self.memory) > batch_size:\n",
    "            minibatch = random.sample(self.memory, batch_size)\n",
    "        else:\n",
    "            minibatch = self.memory\n",
    "\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        self.optimizer.zero_grad()\n",
    "        states, actions, rewards, next_states, is_dones = zip(*minibatch)\n",
    "        states_tensor = torch.tensor(states, dtype=torch.float32).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            targets = torch.tensor(self.get_targets(rewards, next_states, is_dones)).to(DEVICE)\n",
    "        outputs = self.model.forward(states_tensor)\n",
    "        outputs_selected = outputs[torch.arange(len(minibatch)), actions]\n",
    "        loss = F.mse_loss(outputs_selected, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.current_iteration = self.current_iteration + 1\n",
    "        if self.current_iteration % self.target_model_update_iterations == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def select_action_index(self, state, apply_epsilon_random):\n",
    "        if apply_epsilon_random == True and random.uniform(0, 1) < self.epsilon:\n",
    "            return self.actions[np.random.choice(self.number_of_actions)] # phidot, psidot actions\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(np.array(state)[np.newaxis, :], dtype=torch.float32).to(DEVICE)\n",
    "            prediction = self.model(state_tensor)\n",
    "            return self.actions[np.argmax(prediction.detach().cpu().numpy()[0])]\n",
    "\n",
    "    def get_targets(self, rewards, next_states, is_dones):\n",
    "        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            next_states_tensor = torch.tensor(next_states, dtype=torch.float32).to(DEVICE)\n",
    "            q_values_next_states = self.target_model.forward(next_states_tensor)\n",
    "            max_values, _ = torch.max(q_values_next_states, dim=1)\n",
    "            targets = rewards_tensor + self.gamma * max_values # Q-Learning is off-policy\n",
    "            targets = [target if is_done == False else r for r,target,is_done in zip(rewards_tensor,targets,is_dones)]\n",
    "        return targets\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97fb15e6f1c3360a52148bf7d407e0a1ddfa8581bae557519443666d0ee75c56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
